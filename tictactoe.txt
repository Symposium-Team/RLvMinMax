import numpy as np
import random
import matplotlib.pyplot as plt

# Tic-Tac-Toe Game
class TicTacToe:
    def __init__(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1

    def reset(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1

    def is_valid_move(self, row, col):
        return self.board[row][col] == 0

    def make_move(self, row, col):
        if self.is_valid_move(row, col):
            self.board[row][col] = self.current_player
            self.current_player = -self.current_player
            return True
        return False

    def get_available_moves(self):
        return np.argwhere(self.board == 0)

    def is_game_over(self):
        # Check rows
        for row in self.board:
            if abs(np.sum(row)) == 3:
                return True, np.sum(row)

        # Check columns
        for col in self.board.T:
            if abs(np.sum(col)) == 3:
                return True, np.sum(col)

        # Check diagonals
        diag_sum1 = np.trace(self.board)
        diag_sum2 = np.trace(np.fliplr(self.board))
        if abs(diag_sum1) == 3 or abs(diag_sum2) == 3:
            return True, diag_sum1 or diag_sum2

        # Check for a draw
        if len(self.get_available_moves()) == 0:
            return True, 0

        return False, None

# Q-Learning Agent
class QLearningAgent:
    def __init__(self, alpha, gamma, epsilon):
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.epsilon = epsilon  # Exploration rate
        self.q_values = {}  # Q-values table

    def get_q_value(self, state, action):
        state = tuple(map(tuple, state))
        action = tuple(action)
        if (state, action) not in self.q_values:
            self.q_values[(state, action)] = 0.0
        return self.q_values[(state, action)]

    def update_q_value(self, state, action, value):
        state = tuple(map(tuple, state))
        action = tuple(action)
        self.q_values[(state, action)] = value

    def choose_action(self, state, available_moves):
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(available_moves)
        else:
            q_values = [self.get_q_value(state, action) for action in available_moves]
            max_q = max(q_values) if q_values else 0.0
            best_moves = [action for action, q in zip(available_moves, q_values) if q == max_q]
            return random.choice(best_moves)

    def train(self, num_episodes):
        game = TicTacToe()
        for _ in range(num_episodes):
            game.reset()
            state = np.copy(game.board)
            done = False
            while not done:
                available_moves = game.get_available_moves()
                action = self.choose_action(state, available_moves)
                row, col = action
                game.make_move(row, col)
                next_state = np.copy(game.board)
                done, reward = game.is_game_over()

                if done:
                    self.update_q_value(state, action, reward)
                else:
                    max_q_next = max([self.get_q_value(next_state, next_action) for next_action in game.get_available_moves()] or [0.0])
                    reward = reward or 0.0  # Set reward to zero if it's None
                    self.update_q_value(state, action, reward + self.gamma * max_q_next)

                state = next_state

# Monte Carlo Tree Search Agent
class MCTSAgent:
    def __init__(self, num_simulations):
        self.num_simulations = num_simulations

    # ... (implement MCTS agent methods) ...

# Play Tic-Tac-Toe Game between Q-learning and MCTS agents
def play_game(player1, player2):
    game = TicTacToe()
    players = [player1, player2]
    current_player = random.choice(players)

    while True:
        state = np.copy(game.board)
        available_moves = game.get_available_moves()

        if current_player == player1:
            action = player1.choose_action(state, available_moves)
        else:
            # Implement MCTS action selection here
            if len(available_moves) > 0:
                action = random.choice(available_moves)
            else:
                action = None

        if action is None:
            return 0  # Return draw if there are no available moves
        else:
            row, col = action
            game.make_move(row, col)

            done, reward = game.is_game_over()
            if done:
                return reward

        current_player = players[1] if current_player == players[0] else players[0]



# Initialize agents
q_learning_agent = QLearningAgent(alpha=0.5, gamma=0.9, epsilon=0.1)
mcts_agent = MCTSAgent(num_simulations=1000)

# Train Q-learning Agent
q_learning_agent.train(num_episodes=10000)

# Play Tic-Tac-Toe Games
num_games = 100
outcomes = []

for i in range(num_games):
    outcome = play_game(q_learning_agent, mcts_agent)
    outcomes.append(outcome)
    print(f"Game {i+1} outcome: {outcome}")

# Plotting the outcomes
x = np.arange(1, num_games + 1)
plt.plot(x, outcomes)
plt.xlabel('Game')
plt.ylabel('Outcome')
plt.title('Tic-Tac-Toe Game Outcomes')
plt.show()
